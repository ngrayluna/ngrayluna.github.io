[{"authors":["admin"],"categories":null,"content":"Recent M.S. Graduate from Top-tier University with experience in: developing and implementing Deep Learning algorithms for non-traditional applications, working on collaborative projects with international team, processing and handling time-series data, scientific analysis, and experience working at a renowned laboratory. Open for new opportunities and/or collaborations!\n","date":-62135596800,"expirydate":-62135596800,"kind":"taxonomy","lang":"en","lastmod":-62135596800,"objectID":"2525497d367e79493fd32b198b28f040","permalink":"https://ngrayluna.github.io/authors/admin/","publishdate":"0001-01-01T00:00:00Z","relpermalink":"/authors/admin/","section":"authors","summary":"Recent M.S. Graduate from Top-tier University with experience in: developing and implementing Deep Learning algorithms for non-traditional applications, working on collaborative projects with international team, processing and handling time-series data, scientific analysis, and experience working at a renowned laboratory. Open for new opportunities and/or collaborations!","tags":null,"title":"Noah Luna","type":"authors"},{"authors":[],"categories":[],"content":" From earthquake detection to oil discovery, seismic waves allow us to investigate Earth processes and its interior in ways no other data type can.\nTo record these waves we use seismometers. Seismometers are highly sensitive instruments that typically measure translational ground motions. These instruments are sensitive enough to measure large, distant earthquakes from across the globe. For example, a seismometer in California can detect a magnitude 7.0 or above earthquake in Chile, Japan and/or even Turkey!\nGiven their level of sensitivity seismometers also pick up noise. This includes, but is not limited to: instrumental noise, changes in the Earth’s tides, pressure variations, and traffic. Seismologists therefore commonly apply filters to remove background noise, similar to what a sound-engineer does when recording live music. However, knowing which filter to apply is not intuitive. Also, applying a filter might remove relevant information from the signal if the noise is in the same frequency band as the signal itself.\nOver the years there have been statistical efforts to remove noise without tampering the original signal. However, these methods usually require specifying some sort of variable threshold. Incorrectly picking this threshold can lead to false signals[5] [2].\nBecause of the nontrivial task of removing noise from seismic data, geophysicists are on the constant lookout for alternatives approaches. Ideally, a geophysicist would have a set of tools at their disposal that can remove noise from seismic data (or denoise) with minimal human intervention. Additionally, we hope that such a method increases the signal-to-noise ratio of the signal. For the above-mentioned reasons, geophysicists are investigating if deep neural networks can provide new insight and facilitate discoveries.\nIn this post, we will look into one approach in applying Deep Neural Networks to remove noise from seismic data. Table of Contents  Introduction Understanding the Data Model Pre-Processing Results References  Understanding the Data How can we do this? Before moving on the details, we first need to understand digital seismic data. Modern seismometers record the voltage required to recenter a displaced internal mass. A higher voltage means the mas needs to be \u0026ldquo;pushed\u0026rdquo; back further. In other words, the mass was significantly displaced by ground shaking. The voltage measured is recorded as a function of time (in seconds) or sample points (npts). This voltage corresponds to either velocity or acceleration.\nDigital seismic data can be described as a discrete sum of several sinusoids — each with a unique peak amplitude, frequency, and a phase-lag relative alignment [6]. As a result, we can take advantage of tools commonly used for signal processing.\nTypical audio signal processing includes: removing ambient noise, taking into account unequal distribution microphone recording, enhancing certain frequency bands and suppressing others, and so on. In recent years deep neural networks have been used to tackle some of these common processing tasks. You might have already come across these deep neural network algorithms in action if you have used applications with automated speech recognition(ASR) such as Amazon’s Alexa and Apple’s Siri.\nFigure: Logos of Apple’s Siri [left] and Amazon’s Alexa [right]. Both speech recognition systems use deep neural networks for automated speech recognition(ASR).\n\nLike seismic data, one of the challenges behind speech recognition is the presence of background noise, which convolutes the signal [3]. The challenge of removing noise (and thus improving the signal-to-noise ratio) in ASR parallels what the Seismological community faces when removing noise from seismic recordings. Instead of enhancing speech recordings, seismologists desire to enhance earthquake signals.\nModel Before deciding which architecture to use, we did some research on current practices both within and outside the geophysical community. We did this because a) the deep learning field is constantly changing so a popular algorithm today may be shown to be inefficient tomorrow (I am looking at you RNN), b) we want to learn from other people’s mistakes and c) we wanted our approach to be unique.\n[5] approached the problem by first transforming their seismic data into the frequency domain and then using both the real and imaginary parts as input to a convolutional neural network. [3] developed a denoising method based on a parametric dictionary learning scheme. Dictionary learning is a branch of signal processing and machine learning which exploits underlying sparse structures of the input training data. With these sparse representations, they were able to capture the characteristics of seismic data that could then be used for denoising. [1] used a combination of a U-net and incorporating a pre-trained neural network known as ResNet34. In doing so they took advantage of the feature extracting capabilities of convolutional neural networks and the benefits of using pre-trained neural networks.\nAutoencoders are a type of Convolutional Neural Network that are used for unsupervised learning. If trained reliably, an autoencoder takes an input, converts it to a lower dimension, reconstructs the original dimension, then outputs something that looks similar to its original input. Assuming the model is trained to recreate the input, the hidden layer “bottleneck” in the neural network forces the autoencoder to contain information needed to represent the input but at a lower dimension. This makes autoencoders power feature extractors.\nFigure: Denoising autoencoder model architecture used for training. The left portion (outlined in green on the top) depicts the encoder and the right portion (outlined in orange on top) is the decoder. If trained to reconstruct its input reliably, the coding (center dark blue block where the bottleneck occurs) can represent the input but at a lower dimension.\n\nTo prevent the autoencoder from trivially learning its input, the input is corrupted; thereby forcing the autoencoder to learn useful features. To reduce the corruption process, the neural network learns/captures the probabilistic distribution of the input. We call this type of autoencoder, where the input has been corrupted with noise added to it, denoising autoencoders.\nWe defined our denoising autoencoder with alternating convolutional and Max pooling layers for the encoder and alternating upsampling and convolutional layers for the decoder portion of the denoising autoencoders (DAE).\nThere no rules for knowing which hyperparameters to use for a given model. We obtained a ballpark idea of what types of parameters to use by reading literature about similar machine learning projects and through trial-and-error.\nPre-Processing Given the rich history of earthquake monitoring in California, we were able to download seismic data from the Northern California Earthquake Data Center (NCEDC). We chose stations in Northern California that are known to have a high signal-to-noise ratio. We queried events located within a 300km radius around each station. This was done to limit the length of our input training data to five-minute records. We also set a minimum earthquake magnitude threshold so that the starting SNR was large enough to preserve the earthquake signal. With this criterion, we found approximately 10,000 unique station recordings within 18 years.\nEach component of our records (Z, N, E) was treated as a “unique” instance. We thus expanded our training set by a factor of three, giving us approximately 30,000 training instances. Waveforms were detrended, high-passed filtered with a maximum frequency of 0.5Hz, and normalized. We then decimated our traces by a factor of two. This was done to reduce the number of parameters the machine learning algorithm would have to learn and work with during training.\nOur denoising autoencoder needs two inputs: an input x and a corrupted version xx. To create a corrupted version xixi, we downloaded data between earthquake events from stations with poor SNR. We took this noise and added it to a copy of our pre-processed data. This left us with two versions: a clean version and a noisy/corrupted version. The noisy waveform and its associated original clean version are the input and label to our denoiser autoencoder, respectively.\nPreliminary Results To get a taste of our model’s performance we turned to the learning, sometimes called the loss, curve. With each epoch, the predictions made by the algorithm should improve, i.e. the cost should go down. The plot below shows our model’s current performance (as of 07.10.19).\nFigure: Loss curve computed for the training (red curve) and validation (blue curve) set during training from our denoising autoencoder model. Note how the model was stuck at a saddle point between epoch 15 - 210. Model training was terminated when the curves plateaued. This was done to prevent overfitting.\n\nThe cost for both the training and validation set decreased with each epoch. Our validation curve is nosier than our training curve which could suggest we need to modify our batch sizes or that the data within our validation set is semi-irregular and thus leads varying levels of loss per epoch. However, the fact the validation curve has a decreasing trend is promising. Note that the model was stuck at a saddle point about 15 – 210 epoch, thus making our model take longer to improve.\nLearning curves give us insight into whether or not or model is learning, however, it doesn’t tell us much about the time-series output. The denoiser is spitting out waveforms that have (or should have) had noise removed.\nIf the noise has been removed, then we expect the signal-to-noise ratio to increase after the denoiser is applied. Let’s plots this. Below is the SNR before and after our denoiser is applied on the test set:\nFigure: Histogram of the signal-to-noise ratio of the the noisy/corrupted version (yellow) and the denoised version from our DAE (purple) using the pre-trained model on the test set.\n\nNot bad! If we compare the SNR before and after running the model we can see that the mode goes from ~3dB to ~10dB. Very promising.\nWe need to talk about physics before we can celebrate. Seismic data is a means by which we can learn about the interior of the Earth. Thus, we need to check what, if any, of the underlying physics that created the waveforms were preserved from using our denoising autoencoder. Two metrics we can look at are changes in amplitude and changes in phase arrival times (i.e. the onset of the first and second wave arrivals).\nBelow we show the values of the peak (or max) amplitude of the waveforms as a function of their original amplitudes using a linear least-squares polynomial fit. The black curve shows the original amplitude and is used as a reference for us to compare how the amplitudes changed when we added noise (red line) and after it was denoised (purple). Looking at the plot we note that adding noise increases the peak amplitude values for the data set. We would hope to see that using the denoising autoencoder will restore the original amplitude and thus give us a linear trend with a similar slope as the original. However, it would seem that overall the DAE is unable to return the peak amplitude values as can be seen with the purple linear fit curve which has a slope less than one.\nFigure: Max amplitude of the waveforms as a function of their original amplitudes using a linear least-squares polynomial fit. The original, non-corrupted data (black curve), has a slope of 1 (since it is a function of itself). The red and purpled curves show the linear trend of the noisy input (red) and denoised (purple), respectively.\n\nLet\u0026rsquo;s turn our attention to the arrival times of the first seismic wave, the P-wave. To find the p-wave arrival time we used and automated phase picker by Baer et. al on our three data sets: the original waveforms, the noisy (corrupted) waveforms, and the denoised waveforms. We then computed the difference in picked arrival times between the original and the denoised (purple) and the original and the corrupted/noisy waveforms (red line). The results are shown below.\nFigure: Difference in picked arrival times between the original and the denoised (purple) and the original and the corrupted/noisy waveforms (red line). A linear least-squares polynomial was used to fit the data.\n\nOnce again we use linear least-squares polynomial to fit our data and plot it as a function of the SNR before denoising. Though the automated phase picker is not picking the same p-wave arrival as the uncorrupted version (if this was the case the purple line would have a slope of 0), it does, however, pick p-arrivals closer to the true value (as we can see by the smaller differences in arrival time).\nThis is where our model currently stands. We have a few items on our to-do list which we think can help our model. We will go over some possible routes in the next section.\nFuture Work One of the first items on our to-do list is to check whether data augmentation will help our model reach lower cost values and/or converge more quickly. One option could be to modify the noise levels adding to our waveforms. While the information gain won’t be as significant as using new and unique data, it should nonetheless improve the model.\nExpanding our training data set (either by data augmentation or downloading data from other stations) would motivate us to train a model with more hidden layers. It might very well be that a larger data set does a better job of describing the statistical variation in the entire sample space.\nFrom a geophysicist\u0026rsquo;s point of view, there are still a lot of questions that need to be answered. And, from the broader science point of view, there remain questions of what role Machine Learning should have in advancing scientific exploration in the physical sciences. As of writing this post, Deep Neural Networks are showing promising preliminary results. How we evaluate them, how we ensure reproducibility, etc. are topics that remain to be explored and answered.\nReferences [1] Alan Richardson and Caelen Feller. \u0026ldquo;Seismic data denoising and deblending using deep learning\u0026rdquo;. (2019).\n[2] Li-ping Zhang et al. \u0026ldquo;Seismic Signal Denoising and Decomposition Using Deep Neural Networks\u0026rdquo;. In: International Journal of Petrochemical Science \u0026amp; En- gineering (2017).\n[3] Lingchen Zhu, Entao Liu, and James H. McClellan. \u0026ldquo;Seismic data denoising through multiscale and sparsity-promoting dictionary learning\u0026rdquo;. In: Geo- physics 80 (2015).\n[4] Mike Kayser and Victor Zhong. \u0026ldquo;Denoising Convolutional Autoencoders for Noisy Speech Recognition\u0026rdquo;.\n[5] Weiqiang Zhu and Gregory C. Beroza. \u0026ldquo;PhaseNet: A Deep-Neural-Network-Based Seismic Arrival Time Picking Method\u0026rdquo;. In: Geophysics Journal Inter- national 216 (2018), pp. 261-273.\n[6] Öz Yilmaz. \u0026ldquo;Introduction to fundamentals of signal processing\u0026rdquo;. (2001) https://wiki.seg.org/wiki/Introduction_to_fundamentals_of_signal_processing\n","date":1570488802,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1570488802,"objectID":"27e3ac6611b55d4a81d00b5482de3095","permalink":"https://ngrayluna.github.io/project/denoiser/","publishdate":"2019-10-07T15:53:22-07:00","relpermalink":"/project/denoiser/","section":"project","summary":"From earthquake detection to oil discovery, seismic waves allow us to investigate Earth processes and its interior in ways no other data type can.\nTo record these waves we use seismometers. Seismometers are highly sensitive instruments that typically measure translational ground motions. These instruments are sensitive enough to measure large, distant earthquakes from across the globe. For example, a seismometer in California can detect a magnitude 7.0 or above earthquake in Chile, Japan and/or even Turkey!","tags":[],"title":"Removing Noise from Seismic Data with Denoising Autoencoders","type":"project"},{"authors":[],"categories":[],"content":"Introduction Earthquakes are unpredictable. And if you are a Californian Native like me (or happen to live near an active earthquake fault), chances are you have felt ground shaking caused by an earthquake. There is no algorithm to predict earthquakes. However, the physics governing seismic wave propagation allow us to prepare ourselves from shaking once an earthquake has occured.\nSeismic waves travel slower than light (i.e. the speed at which emails, txts, travel). It is therefore possible to send a warning about incoming ground motion shaking after an earthquake has occured. This is the main idea behind Earthquake Early Warning. The idea being that, if we are a certain distance away from the earthquake hypocenter, we could send a warning (somewhere between seconds to a minute or two) via a txt, email, app. etc to someone else who is in the direction of the incoming seismic waves.\nIn order to send an earthquake early warning, however, seismologists need to have a high level of certainty that: 1) the ground motion recorded by a seismometer is in fact an earthquake and not background noise (e.g. someone walking near a sensor) and 2) the location of the earquake.\nLuckily we have computers to accomplish this. Unfortunately, typical detection systems take about a minute to send an earthquake alert. This is partly due to the fact that it takes time to both: get enough information from several seismometers and process the data.\nCan Machine Learning, specifically Deep Learning, give us an alternative approach? An approach which is more accurate and/or faster? More specifically, can we train a deep neural network to help us identify features in seismic records which will allow us determine if an earthquake has occured and if yes, where is it?\nTo make OUR grandiose problem more manageable, we will boil this problem down to a much simpler one:\n\u0026ldquo;Can we train a deep neural network to identify the first seismic wave recorded on a seismometer?\u0026rdquo;\nIn other words, let\u0026rsquo;s assume we have already identified the seismic record is in fact an earthquake. Our task is to now identify the location of the earthquake. To do this we need to know when the incoming wave was first spotted. In other words, we need to know when the first seismic phase has arrived. This phase is known as the Primary Phase (or P-wave) and is marked with a red vertical line in the image below.\nFigure: Earthquake recorded by a seismometer in Wettzell, Germany. The red vertical line notes the Primary Phase, the first seismic phase recorded by the seismometer. The blue vertical line depicts another type of surface phase known as the Secondary Phase. To identify the phases a human (or computer) is looking for a change in both amplitude and frequency. The x axis is in units of number of samples (npts).\n\nThe p-wave travels the faster than other phases (e.g. the S-phase shown above), however the frequency and the polarization (direction) of these waves tends to not cause structural damage (depending on how large the earthquake is, of course). The more destructive seismic phases, known as surface waves, take longer to arrive than the p-wave. The further you are from the earthquake, the larger the time lag between the p-wave and these destructive surface waves. Thus, if we can detect the p-wave (and accurately) we can then compute how much time a person has before they should expect ground motion shaking.\nIn this two-part tutorial we will establish the following machine learning workflow:\nDownload seismic data → Pre-process data \u0026amp; format it for training → Decide which DNN to use and create it → Train model → Analyze Results\nFor the sake of making the tutorial a little more digestable, we will split the tutorial into two parts. Part I (this post) will focus on: framing the problem, how to download data, and pre-processing the data we have downloaded.\nTable of Contents  Introduction\n Framing the Problem Downloading Seismic Data\n Pre-Process\n Saving the Data Summary  Framing the Problem Let\u0026rsquo;s begin! Before we download data let\u0026rsquo;s go over the details of how we will accomplish our objective. Remember our goal is to identify where the first seismic phase, i.e. the p-wave, is in our records. Our plan of attack will be to use a Convolutional Neural Network (CNN). Convolutional neural networks are a type of deep neural network that handle data that is either time or spatially dependent, such as seismograms (time-dependency) or images (spatial dependency) just to name a couple of examples.\nOur data will consist of seismograms of earthquake events recorded by seismometers in California. The seismic records we collect and process are the input to our CNN. Our labels will consist of scalar-valued, p-wave picks made by seismologists (e.g. the point the vertical line crosses on the x-axis in the image above). Thus, for each training instance, we will give our CNN a seismic record (time-series array) as input, x, and its associated p-wave location as its label, y.\nDuring training our neural network will try to identify which features are relevant in the waveforms that allow it to accurately determine where the p-wave. Visually, we as humans can tell where this is by looking in a sudden chance in both frequency and amplitude.\nIn our case we have two different options of how we can feed our CNN our data. We can either feed it images or we can keep the data in arrays. If we choose the former, we will need to define a 2D convolutional neural network. Alternatively, we can use a 1D CNN and keep our data in arrays. Since I want to have as output arrays for further processing, I’ll keep my data in its original 1D format, as it will make future analysis easier.\nGroovy. We now have a more solid outline of what we need to do. Let’s now download some data!\nDownloading Seismic Data Now that we have a gameplan, let’s get our hands dirty and download some seismic data. Luckily for us, the vast majority of earthquake data centers are open and free for public use. In California there are two well-known data centers, the Northern California Earthquake Data Center and the Southern California Data Center.\nWe’ll use data recorded from stations in Northern California. You can find a list of stations, their state of health, and history from the NCEDC website.\nFigure: Subset of seismic stations in Northern California. Yellow circles depict broadband stations and blue depict borehole stations. Courtesy of the NCEDC.\nI will pick some of my personal favorite stations for this tutorial, but feel free to check out others and see what their data looks like in comparison. There is a nifty Python library known as ObsPy which makes accessing seismic data quick and easy. Let\u0026rsquo;s import ObsPy along with some other Python libraries and check out how our data looks like:\nimport os import numpy as np import pandas as pd from obspy.clients.fdsn import Client from obspy import Stream, UTCDateTime from obspy.core.event import read_events import matplotlib.pyplot as plt plt.style.use('ggplot')  Let’s look at how one of these events look like as recorded by a broadband seismometer. You\u0026rsquo;ll notice that in this example I have picked a specific network, station, and channels. Feel free to use * to specify all in ObsPy. This will return a list of several networks, stations, locations, and channels to use for the time duration specified for start time and end time parameters.\nFor the sake of having a \u0026lsquo;nice\u0026rsquo; waveform to look at, I\u0026rsquo;ll manually specify the time of a known earthquake event. (There are more clever ways of querying for earthquakes, but let’s keep it simple and look at just one example). Specifically, I\u0026rsquo;ll download a recent (as of writting this post) earthquake off the cost of San Francisco.\nnetwork = \u0026quot;BK\u0026quot; station = \u0026quot;CMB\u0026quot; location = \u0026quot;*\u0026quot; channel = \u0026quot;BH*\u0026quot; starttime = UTCDateTime(\u0026quot;2019-10-05T15:40:00\u0026quot;) endtime = starttime + 300 client = Client(\u0026quot;NCEDC\u0026quot;) stream = client.get_waveforms(network = network, station = station, location = location,\\ channel = \u0026quot;BH*\u0026quot;, starttime=starttime, endtime=endtime)  Quick Note About ObsPy's Stream Object The following is directly from ObsPy\u0026rsquo;s Documentation page: Seismograms of various formats (e.g. SAC, MiniSEED, GSE2, SEISAN, Q, etc.) can be imported into a Stream object using the read() function.\nStreams are list-like objects which contain multiple Trace objects, i.e. gap-less continuous time series and related header/meta information.\nEach Trace object has the attribute data pointing to a NumPy ndarray of the actual time series and the attribute stats which contains all meta information in a dict-like Stats object. Both attributes starttime and endtime of the Stats object are UTCDateTime objects. A multitude of helper methods are attached to Stream and Trace objects for handling and modifying the waveform data.\u0026rdquo;\nFor example, for each recording our station records motion in three different directions: a vertical motion (up-down) and two horizontal motions (N-S and E-W). Thus, our Stream object has three Traces.\nstream  3 Trace(s) in Stream: BK.CMB.00.BHE | 2019-10-05T15:40:00.019538Z - 2019-10-05T15:44:59.994538Z | 40.0 Hz, 12000 samples BK.CMB.00.BHN | 2019-10-05T15:40:00.019538Z - 2019-10-05T15:44:59.994538Z | 40.0 Hz, 12000 samples BK.CMB.00.BHZ | 2019-10-05T15:40:00.019538Z - 2019-10-05T15:44:59.994538Z | 40.0 Hz, 12000 samples  Each Trace object has head information which we can access by printing the attributes:\ntrace0 = stream[0] print(trace0.stats)   network: BK station: CMB location: 00 channel: BHE starttime: 2019-10-05T15:40:00.019538Z endtime: 2019-10-05T15:44:59.994538Z sampling_rate: 40.0 delta: 0.025 npts: 12000 calib: 1.0 _fdsnws_dataselect_url: http://service.ncedc.org/fdsnws/dataselect/1/query _format: MSEED mseed: AttribDict({'dataquality': 'D', 'number_of_records': 7, 'encoding': 'STEIM2', 'byteorder': '\u0026gt;', 'record_length': 4096, 'filesize': 81920})  We\u0026rsquo;ll use this information to make some useful unit conversions in our plots.\nsampling_rate = trace0.stats.sampling_rate npts = trace0.stats.npts delta = trace0.stats.delta time = np.arange(0, npts / sampling_rate, delta)  Plotting this with Python\u0026rsquo;s Matplotlib we have:\nrows = 3 fig, axes = plt.subplots(nrows = 3, ncols = 1, figsize = (16,10)) axes[0].plot(time, stream[0], color = 'black') axes[1].plot(time, stream[1], color = 'black') axes[2].plot(time, stream[2], color = 'black') axes[rows - 1].set_xlabel('Time [s]')  Text(0.5,0,'Time [s]')  In addition to the training data we will need to also provide labels to our CNN. We are interested in training a CNN to identify the first arriving seismic phase, the p-wave. We can manually identify this by looking at the waveform and searching for a time where both the frequency and amplitude of the waveform changes drastically. In our example we can see this occurring at around 30 seconds. This is the label we will provide to our neural network.\nManually picking our entire data set is something we would have to do IF it were not for the fact that the NCEDC has had individuals manually pick earthquakes for decades and provided it for all to use. We’ll take advantage of their hardwork and simply make a query to download waveforms and their associated p-phase arrival time and store it into a Pandas DataFrame.\nTo find the manually picked phase arrival we\u0026rsquo;ll search through the archived catalogue of events using ObsPy\u0026rsquo;s get_events() method.\n# Minimum magnitude earthquake to search for minmag = 3.0 # Maximum search radius around station to look for events maxrad = 2.697 stla = 38.03 stlo = -120.39 cat = client.get_events(starttime=starttime, endtime=endtime, latitude=stla, longitude=stlo,\\ maxradius=maxrad, minmagnitude=minmag,\\ includearrivals = True) print(cat)  1 Event(s) in Catalog: 2019-10-05T15:41:06.570000Z | +37.660, -122.521 | 3.54 Mw | manual  station_picks = [] for n, event in enumerate(cat): origin_info = event.origins[0].arrivals for i, pick in enumerate(event.picks): onset = pick.onset polarity = pick.polarity evaluation_mode = pick.evaluation_mode evaluation_status = pick.evaluation_status phase_time = pick.time arrival = origin_info[i] pick_id = pick.resource_id pid = arrival['pick_id'] if pid != pick_id: new_arrival = \\ [arrival for arrival in origin_info if arrival['pick_id'] == pick_id] if len(new_arrival) \u0026lt; 1: continue phase = new_arrival[0]['phase'] else: phase = arrival['phase']  # EventID eventID = event.resource_id.id.split('/')[-1] # Let's convert from UTC to something we can work with p_arrival = abs(stream[0].stats.starttime.timestamp - phase_time.timestamp) print(\u0026quot;EventID {}\u0026quot;.format(eventID)) print(\u0026quot;First seismic wave arrives {} seconds from the start of our record.\u0026quot;.format(round(p_arrival, 3)))  EventID 73286930 First seismic wave arrives 91.86 seconds from the start of our record.  Let\u0026rsquo;s plot this to see how this looks like:\ntime = np.arange(0, stream[0].stats.npts / stream[0].stats.sampling_rate, stream[0].stats.delta ) fig, axes = plt.subplots(nrows = 1, ncols = 1 , figsize = (16,6)) axes.plot(time, stream[0].data, color = 'black') axes.axvline(p_arrival, color = 'red') axes.set_xlabel(\u0026quot;Time [s]\u0026quot;)  Text(0.5,0,'Time [s]')  As expected the p-arrival is pick made by the human analyst was made when there was a change in amplitude and frequency. In this case this happens at around 350 seconds.\nPre-Processing If you have experience with signal processing, you already know there are a slew of tools to process audio recordings. We can use similar processing tools for seismic data. In particular, we’ll want to remove trends and filter out unwanted noise. Let’s do that with some of Stream\u0026rsquo;s useful methods:\n# Both the detrend() and filter() operation is performed in place of the actual arrays. # Let's make a copy of this Stream in order to have access to the original. st = stream.copy() # Detrend st.detrend() # Apply high pass filter max_freq = (1/2) st.filter(type=\u0026quot;highpass\u0026quot;, freq = max_freq)  3 Trace(s) in Stream: BK.CMB.00.BHE | 2019-10-05T15:40:00.019538Z - 2019-10-05T15:44:59.994538Z | 40.0 Hz, 12000 samples BK.CMB.00.BHN | 2019-10-05T15:40:00.019538Z - 2019-10-05T15:44:59.994538Z | 40.0 Hz, 12000 samples BK.CMB.00.BHZ | 2019-10-05T15:40:00.019538Z - 2019-10-05T15:44:59.994538Z | 40.0 Hz, 12000 samples  Plotting the waveforms after applying our filters you\u0026rsquo;ll notice that the traces have less noise before the p-wave and after the signal trails off.\nrows = 3 fig, axes = plt.subplots(nrows = rows, ncols = 1, figsize = (16,10)) axes[0].set_title(\u0026quot;Event ID {}\u0026quot;.format(eventID)) axes[0].plot(time, st[0], color = 'black') axes[1].plot(time, st[1], color = 'black') axes[2].plot(time, st[2], color = 'black') axes[rows - 1].set_xlabel(\u0026quot;Time [s]\u0026quot;) for i in range(rows): axes[i].axvline(p_arrival, color = 'red')  Saving the Data There are several file format options we have at our disposal. To keep things simple, we\u0026rsquo;ll use NumPy\u0026rsquo;s array method np.savez_compressed(), which is NumPy\u0026rsquo;s way of saving arrays into a compressed file format with file extension .npz. Once we have done a mass download of our waveforms, we\u0026rsquo;ll save our NumPy arrays using somthing like:\nnp.savez_compressed(array, file_name)\nwhere array is a NumPy array with all of our seismograms and file_name is the name of the output file our NumPy arrays are saved in. For example, in this tutorial I used:\nnp.savez_compressed(\u0026lsquo;./dataset_3comp.npz\u0026rsquo;, clean_data_set)\nWe\u0026rsquo;ll also need to make sure we save a separate NumPy array which will contains our labels, the p-wave picks. For example:\nnp.savez_compressed(\u0026lsquo;./dataset_3comp_cleanlabels.npz\u0026rsquo;, clean_labels)\nSee the associated GitHub repository for the more technical details on how I did this.\nSummary In this tutorial we formally stated the problem, broke down a complicated problem into a more manageable one, downloaded time-series data and manually picked phase information, processed the data by applying filters and demeaning it, and we ended by saving it into a compressed file format (.npz). In the next blog we will go over how to: read in saved training data labels, format the data for training, build a baseline CNN model, train said model, make first-and make first-order interpretations.\nSuggest Reading Material  Ackermann, Nils. \u0026ldquo;Introduction to 1D Convolutional Neural Networks in Keras for Time Sequences\u0026rdquo; Medium, 04 September 2018, https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf\n Géron, A. (2017). Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques for Building Intelligent Systems. O\u0026rsquo;Reilly UK Ltd.\n Information on Earthquake Early Warning provided by the U.C. Berkeley Seismological Laboratory\n Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition.\n  ","date":1569906866,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569906866,"objectID":"eb3513935046a9981be0b151b06a8a74","permalink":"https://ngrayluna.github.io/post/p-phase-picker-tutorial_pi/","publishdate":"2019-09-30T22:14:26-07:00","relpermalink":"/post/p-phase-picker-tutorial_pi/","section":"post","summary":"Introduction Earthquakes are unpredictable. And if you are a Californian Native like me (or happen to live near an active earthquake fault), chances are you have felt ground shaking caused by an earthquake. There is no algorithm to predict earthquakes. However, the physics governing seismic wave propagation allow us to prepare ourselves from shaking once an earthquake has occured.\nSeismic waves travel slower than light (i.e. the speed at which emails, txts, travel).","tags":[],"title":"Identifying Seismic Waves with Convolutional Neural Networks [Part I]","type":"post"},{"authors":[],"categories":[],"content":" In Part I we covered the first steps of our machine learning pipeline: Framing the Problem, Retrieving the Data , Exploring and Understanding your Data, and Processing the Data for training .\nIn this tutorial we will go sraight into compiling, training, and evaluating a baseline convolutional neural network. We\u0026rsquo;ll end by going over what still needs to be done before we can consider this model ready for deployment.\nTable of Contents  Read Data In A Convolutional Neural Network Baseline Training Model Interpreting the Results Visualizing the Results What\u0026rsquo;s Next Conclusion  \nLet\u0026rsquo;s get started! Open a Jupyter Notebook or your favorite Python IDE and include the following modules and functions:\nimport os import numpy as np import pandas as pd from random import randint from keras.layers import Input, Dense from keras.layers import Conv1D, MaxPooling1D, UpSampling1D from keras.layers.normalization import BatchNormalization from keras.layers import Dropout, Activation, Flatten from keras.models import Model from keras.models import model_from_json import matplotlib.pyplot as plt plt.style.use('ggplot')  def form_WAVsaved_npz(data_array): tmp_list = [] for array in data_array: tmp_list.append(data_array[array]) # Convert to Numpy array # (# of instances, # of features) data_array = np.array(tmp_list) # From (1, 7156, 3, 1800) to (7156, 3, 1800) data_array = data_array.reshape(data_array.shape[1], data_array.shape[2], data_array.shape[3]) return data_array def format_Psaved_npz(label_array): tmp_list = [] for array in label_array: tmp_list.append(label_array[array]) # Convert to Numpy array # (# of instances, # of features) data_array = np.array(tmp_list) # (1, #samples, labels) labels = data_array.reshape(data_array.shape[1]) return labels  \nRead Data In\nFor this model, we are using the data saved from our pre-processing efforts in the last tutorial. Recall that in the last post we saved our data (time-series waveforms) and their associated labels (arrival times of the first arriving P-Phase seismic wave) into Numpy\u0026rsquo;s compressed file format.\nThere are two files to read in: time-series waveforms and the arrival times of the first arriving (P-Phase) seismic wave. The latter of which are the labels we are using for our training model. Simply use np.load() to read both .npz files in. Once the files are in memory we\u0026rsquo;ll store them in NumPy arrays. I\u0026rsquo;ve written a function to then take this numpy.lib.npyio.NpzFile and store it into a NumPy array.\n# Let's read this in and check it is right. data_waves = np.load(\u0026quot;./pre_process/waveforms.npz\u0026quot;) data_labels = np.load(\u0026quot;./pre_process/labels.npz\u0026quot;) # labels data_array = form_WAVsaved_npz(data_waves) p_arrivals = format_Psaved_npz(data_labels)  To make our lives easier we’ll also assign the number of traces, the number of features, and the number of sample points to variables num_traces, num_feat, and npts, respectively. num_traces, num_feat, and npts, respectively.\n# Number of traces num_traces = data_array.shape[0] # e.g. (1, 5, 3) # Index of feature we want. num_feat = data_array.shape[2] # npts npts = data_array.shape[1]  At this point, we need to set aside some of our data for training and set aside the remaining data for our validation set. For “smaller” data sets it is common to use 80% of your data set for training and set aside the other 20% for validation. If you have a data set in the millions, then you’ll probably end up using 10% of your data for validation and the rest for training.\nTo split our data set we will set\nTRAIN_TEST_SPLIT = 0.2\nIn other words, we\u0026rsquo;re keeping 80% for training and the remaining 20% for validation.\n# Split data into training and validation # The percent of data that will be included in the test set (here 20%) TRAIN_TEST_SPLIT = 0.2 # TRAIN/VAL split nr_val_data = int(num_traces * TRAIN_TEST_SPLIT) x_train = data_array[nr_val_data:, :] # x composed of two traces y_train = p_arrivals[nr_val_data :] # y values has the s-arrival time x_val = data_array[:nr_val_data, :] y_val = p_arrivals[: nr_val_data]  A Convolutional Neural Network Baseline So far we have read our data into our notebook, formatted it into NumPy arrays, and we just split the data into a training and validation training set. Let’s now define our deep neural network! As stated in the title of this blog and the previous post, we will be using convolutional neural networks (CNN). Among other things, CNN are designed to handle grid-like data such as time-series data and images. Alternatively, we could use a Recurrent Neural Network (RNN) which can also handle data with spatial-temporal dependencies. Let’s stick to CNNs as they are currently one of the most successful models in the field of Computer Vision.\nTo create our convolutional neural network model we will use Keras which is a high-level neural network API. It is fairly straight forward to use and the community supporting Keras is robust.\n\nHyperparameter Choice\nWe are now ready to define some hyperparameters. Unfortunately, there does not exist a set of rules which will tell you what hyperparameters you should use. The process of fine-tuning your model is empirical and requires a lot of trial and error. However, that doesn’t mean you should start choosing randomly. For a starting point, read up on current best practices, learn what model configurations are working for other groups, and do a little homework on which parameters are appropriate. I\u0026rsquo;ve gone ahead and done this for this tutorial. With that said, we\u0026rsquo;ll be using the following hyperparameters:\n Mini batches of 256 [memory on computers is stored in 2\u0026rsquo;s, so batches of a power of 2 helps train a little faster]\n 400 epochs [let\u0026rsquo;s just start with this]\n Filters size of 32 and 64\n Kernel size of size 6 Pool size of 2\n Rectified linear unit (ReLU) activation function for the hidden layers Linear activation function for the output layer [we want an activation function which can give us values between -1 to 1] Mean squared error loss function\n Adam optimizer [best of RMSprop and gradient decent with momentum]  If you have experience using convolutional neural networks, you’ll already know that in addition to specifying the input size, we also need to define the number of channels. Thus, our input will have the form of:\n(# training examples, number of samples, # of channels).\nSeismometers record motion in three directions: a vertical(up-down) and two horizontal motions (N-S and E-W). Thus, for each training example, we will give our neural network not just one, but three waveforms. Thus, our input array will have three channels. Below is an illustration of what our CNN looks like:\nFigure: Schematic of the 1D convolutional neural network used to identify the first-arriving phase arrival of an earthquake. Note that each instance is composed of three channels, one for each component measured by the seismometer (Vertical, North-South, East-West).\nFor the sake of brevity, I’ll omit commentary on how we chose the number of hidden layers and the filter size (sometimes referred to as ‘kernels’) in this blog. I’ll make a separate blog detailing the nitty-gritty details of how convolutional neural networks work.\nWith this in mind let’s go ahead and make our convolutional neural network. Below is a summary of our model. Each line describes a hidden layer and it’s associated number of weights and parameters. Our input array, x_train, has input dimension of:\nwhich we store in as a variable labeled input_trace.\n# Hyperparameters batch_size = 256 epochs = 400 filters = [32, 64] kernel_size = 6 pool_size = 2 padding = 'same' hidden_act_fun = 'relu' final_act_fun = 'linear' optimizer = 'adam' loss_type = 'mean_squared_error' name = 's_phase_picker' ## MODEL ## # Input placeholder input_trace = Input(shape=(x_train.shape[1], x_train.shape[2])) x = Conv1D(filters = filters[0], kernel_size = kernel_size, activation=hidden_act_fun, padding = padding)(input_trace) x = MaxPooling1D(pool_size = pool_size, padding = padding)(x) x = Conv1D(filters = filters[1], kernel_size = kernel_size, activation=hidden_act_fun, padding = padding)(x) x = MaxPooling1D(pool_size = pool_size, padding = padding)(x) cnn_feat = Flatten()(x) x = Dense(units= 32, activation=hidden_act_fun)(cnn_feat) x = BatchNormalization()(x) x = Dense(units = 8, activation=hidden_act_fun)(x) x = BatchNormalization()(x) dense = Dense(units= 1, activation=final_act_fun)(x) # Compile Model p_phase_picker = Model(input_trace, dense, name = name) p_phase_picker.compile(optimizer = optimizer, loss = loss_type) p_phase_picker.summary()  _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= input_1 (InputLayer) (None, 1800, 3) 0 _________________________________________________________________ conv1d_1 (Conv1D) (None, 1800, 32) 608 _________________________________________________________________ max_pooling1d_1 (MaxPooling1 (None, 900, 32) 0 _________________________________________________________________ conv1d_2 (Conv1D) (None, 900, 64) 12352 _________________________________________________________________ max_pooling1d_2 (MaxPooling1 (None, 450, 64) 0 _________________________________________________________________ flatten_1 (Flatten) (None, 28800) 0 _________________________________________________________________ dense_1 (Dense) (None, 32) 921632 _________________________________________________________________ batch_normalization_1 (Batch (None, 32) 128 _________________________________________________________________ dense_2 (Dense) (None, 8) 264 _________________________________________________________________ batch_normalization_2 (Batch (None, 8) 32 _________________________________________________________________ dense_3 (Dense) (None, 1) 9 ================================================================= Total params: 935,025 Trainable params: 934,945 Non-trainable params: 80 _________________________________________________________________  Training the Model With our data read in and our model architecture defined, we are now ready to run our baseline convolutional neural network. With Keras, this amounts to giving our compiled model the training data set and its associated labels, the number of epochs and batch size we want, and the validation (see below). To train our CNN we use the model\u0026rsquo;s .fit() method:\n# Train Model history = p_phase_picker.fit(x = x_train, y = y_train, epochs= epochs, batch_size=batch_size, validation_data=(x_val, y_val)) # Keep track of the last loss values (for easy comparison later) train_loss = history.history['loss'] last_train_loss_value = train_loss[len(train_loss)-1] val_loss = history.history['val_loss'] last_val_loss_value = val_loss[len(val_loss) - 1]  Epoch 1/400 5725/5725 [==============================] - 4s 780us/step - loss: 367974.3419 - val_loss: 370470.2668 Epoch 2/400 5725/5725 [==============================] - 4s 632us/step - loss: 367804.4836 - val_loss: 371916.8610 Epoch 3/400 5725/5725 [==============================] - 4s 624us/step - loss: 367594.1336 - val_loss: 370046.7134 Epoch 4/400 5725/5725 [==============================] - 4s 635us/step - loss: 367357.8737 - val_loss: 370197.6924 Epoch 5/400 5725/5725 [==============================] - 4s 647us/step - loss: 367116.8329 - val_loss: 372166.2775 Epoch 6/400 5725/5725 [==============================] - 4s 645us/step - loss: 366859.2670 - val_loss: 371878.3604 Epoch 7/400 5725/5725 [==============================] - 4s 651us/step - loss: 366575.9700 - val_loss: 368855.4593 Epoch 8/400 5725/5725 [==============================] - 4s 644us/step - loss: 366252.4366 - val_loss: 365949.0822 Epoch 9/400 5725/5725 [==============================] - 4s 643us/step - loss: 365918.8507 - val_loss: 353128.8180 Epoch 10/400 5725/5725 [==============================] - 4s 640us/step - loss: 365568.8157 - val_loss: 360134.3884 Epoch 11/400 5725/5725 [==============================] - 4s 643us/step - loss: 365160.4010 - val_loss: 362235.5229 Epoch 12/400 5725/5725 [==============================] - 4s 641us/step - loss: 364738.2535 - val_loss: 352015.1630 Epoch 13/400 5725/5725 [==============================] - 4s 641us/step - loss: 364275.2614 - val_loss: 354300.4198 Epoch 14/400 5725/5725 [==============================] - 4s 634us/step - loss: 363814.7165 - val_loss: 347660.5830 Epoch 15/400 5725/5725 [==============================] - 4s 636us/step - loss: 363339.9869 - val_loss: 357162.0319 Epoch 16/400 5725/5725 [==============================] - 4s 640us/step - loss: 362818.9280 - val_loss: 359878.0607 Epoch 17/400 5725/5725 [==============================] - 4s 637us/step - loss: 362306.8130 - val_loss: 348699.6071 Epoch 18/400 5725/5725 [==============================] - 4s 640us/step - loss: 361731.6002 - val_loss: 349205.2539 Epoch 19/400 5725/5725 [==============================] - 4s 636us/step - loss: 361150.4833 - val_loss: 351060.9939 Epoch 20/400 5725/5725 [==============================] - 4s 635us/step - loss: 360559.7594 - val_loss: 340123.2790 . . . Epoch 385/400 5725/5725 [==============================] - 4s 637us/step - loss: 6572.9224 - val_loss: 18183.6428 Epoch 386/400 5725/5725 [==============================] - 4s 635us/step - loss: 6540.7187 - val_loss: 18267.0756 Epoch 387/400 5725/5725 [==============================] - 4s 635us/step - loss: 6536.7624 - val_loss: 18081.5158 Epoch 388/400 5725/5725 [==============================] - 4s 634us/step - loss: 6536.4518 - val_loss: 18305.2634 Epoch 389/400 5725/5725 [==============================] - 4s 637us/step - loss: 6549.2467 - val_loss: 18346.6905 Epoch 390/400 5725/5725 [==============================] - 4s 639us/step - loss: 6537.4118 - val_loss: 18083.2445 Epoch 391/400 5725/5725 [==============================] - 4s 635us/step - loss: 6526.0619 - val_loss: 18387.9119 Epoch 392/400 5725/5725 [==============================] - 4s 639us/step - loss: 6508.7915 - val_loss: 18279.9480 Epoch 393/400 5725/5725 [==============================] - 4s 633us/step - loss: 6522.6132 - val_loss: 18310.2310 Epoch 394/400 5725/5725 [==============================] - 4s 635us/step - loss: 6518.6792 - val_loss: 18221.4595 Epoch 395/400 5725/5725 [==============================] - 4s 636us/step - loss: 6532.9096 - val_loss: 18169.3012 Epoch 396/400 5725/5725 [==============================] - 4s 638us/step - loss: 6525.6210 - val_loss: 18193.7779 Epoch 397/400 5725/5725 [==============================] - 4s 636us/step - loss: 6519.6705 - val_loss: 18258.7685 Epoch 398/400 5725/5725 [==============================] - 4s 632us/step - loss: 6517.0802 - val_loss: 18250.5715 Epoch 399/400 5725/5725 [==============================] - 4s 633us/step - loss: 6516.5765 - val_loss: 18335.3135 Epoch 400/400  Interpreting the Results What does the above tell us? For each epoch the Keras API prints:\n1) Computation time\n2) The loss from the training and validation data sets.\nThe second point is worth discussing. Remember that the overall objective is to create an algorithm that learns from the data we give it. i.e. we want our algorithm to generalize to data it has never seen before. We should expect, therefore, that the training loss decreases for every epoch. Does this happen in our case? Let’s plot the training and validation loss curves (sometimes called ‘learning curves’) to help us understand a little more of how well our deep neural network performed:\n# Validation loss curves # fig, axes = plt.subplots(nrows = 1, ncols = 1, figsize = (10,6)) axes.plot(history.history['loss']) axes.plot(history.history['val_loss']) axes.set_title('Model Loss') axes.set_ylabel('loss') axes.set_xlabel('epoch') axes.legend(['Train', 'Validation'], loc='upper left')  Figure: The learning curve results from training the 1D convolutional neural network with the above-mentioned hyperparameters. The red curve depicts loss from the training set, the blue curve depicts loss from the validation set.\nThe red and blue curves show the loss (sometimes called ‘cost’) per epoch on the training and validation set, respectively. As expected, the neural network performs poorly at the onset of training (at this point the model is probably randomly guessing where the first phase is) and gradually improves with epoch. During training the neural network is learning directly from the training set, so the prediction error (loss/cost) is lower than that of the validation training set.\nWe stopped training around epoch 400 because at this point it would seem our neural network is no longer learning/improving its ability to make predictions. Also, we don’t want to let the neural network run too long, else the neural network might begin to overfit.\nVisualize the Results I don’t know about you, but most of the tutorials I come across normally end here. Meaning, they show a loss curve from training and call it a day. I am a visual person, so I want to see the output of my model. Sure, these learning curves tell me that, to a first-order that my neural network is learning to pick the first arrival. But, how does this look compared to the pick made by a human being? Let’s look at some earthquakes.\nBelow are two randomly (using Numpy\u0026rsquo;s rand.randint() function) chosen waveforms from our validation set. The red vertical line is the pick made by a human. The purple line is the pick made by our CNN.\ndef read_results(file_name): file_npz = np.load(file_name) read_list = [] for item in file_npz: read_list.append(file_npz[item]) return np.array(read_list)  directory = './data_directory' ## Read in earthquake waveforms file = \u0026quot;waves4picking.npz\u0026quot; file_path = os.path.join(directory, file) waves_array = read_results(file_path) # Reshape from (1, 1431, 1800, 3) to (1431, 1800, 3) waves_array = np.reshape(waves_array, newshape=(waves_array.shape[1], waves_array.shape[2], waves_array.shape[3])) ## Read picks made by people file2 = \u0026quot;spicks_used.npz\u0026quot; file_path2 = os.path.join(directory, file2) ppick_array = read_results(file_path2) ppick_array = np.reshape(ppick_array, newshape=(ppick_array.shape[1])) # Reshape from (1, 1431) to (1431,) ## Picks made by machine learning file3 = 'ml_spicks.npz' file_path3 = os.path.join(directory, file3) ml_ppicks = read_results(file_path3) ml_ppicks = np.reshape(ml_ppicks, newshape=(ml_ppicks.shape[1])) # Reshape from (1, 1431) to (1431,)  # Waveform specs. sampling_rate = 20.0 npts = 1800 delta = 0.05 time = np.arange( 0, npts / sampling_rate, delta)  # Let's select waveforms, and their associated picks, randomly tr = np.random.randint(0, waves_array.shape[0]) tr1 = np.random.randint(0, waves_array.shape[0]) rows = 2 fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(16,8)) axes[0].set_title(\u0026quot;Trace {}\u0026quot;.format(tr)) axes[0].plot(time, waves_array[tr, :, 0], color = 'black') axes[0].axvline(ppick_array[tr] / sampling_rate, color = 'red', label = \u0026quot;p_pick manual\u0026quot;) axes[0].axvline(ml_ppicks[tr] / sampling_rate, color = 'purple' , label = 'ml_pick') axes[1].set_title(\u0026quot;Trace {}\u0026quot;.format(tr1)) axes[1].plot(time, waves_array[tr1, :, 0], color = 'black') axes[1].axvline(ppick_array[tr1] / sampling_rate, color = 'red', label = \u0026quot;p_pick manual\u0026quot;) axes[1].axvline(ml_ppicks[tr1] / sampling_rate, color = 'purple' , label = 'ml_pick') axes[1].set_xlabel(\u0026quot;Time [s]\u0026quot;) for i in range(rows): axes[i].set_ylabel('Norm. Amp.') axes[i].legend() plt.tight_layout()  Figure: Two randomly chosen earthquake seismic records from the training set. The red vertical line is the manually selected phase arrival. The purple line depicts the phase pick made by the machine learning algorithm.\n\nYou’ll notice that, depending on the waveform, our CNN has varying levels of success in picking the first arrival. This makes sense given that so far we have only run a baseline model; there is still plenty of refining and polishing that has to be done before we can expect too much from our neural network.\nWhat\u0026rsquo;s left do? We\u0026rsquo;ll explore this in the next section.\nWhat's Next? There is a lot to do before we can call this model satisfactory. For a start, we should try to get more data. The model results are shown above only used ~10,000 waveforms, which is a bit on the small side for a deep neural network. It might be instructive to try to double or triple the data set and see if our loss curves improve.\nWe could also play with the complexity of our model. At the moment we only have two, 1D convolutional layers. Perhaps the model will perform better with more or larger filter sizes? (Warning: it is best practice to start small before jumping into larger models!)\nOne could and should explore the batch sizes and/or using a different optimization method.\nWe could and should also shift the window where our seismic waveform is centered. Not only might this make our neural network more robust, but it will also increase how much training data we have (i.e. data augmentation).\nOnce we have reached a point where we are satisfied with how our model is performing we will need to run this on a test set. In other words, we need to run this on a data set which the convolutional neural network has never seen before. We could easily achieve this by setting aside test data when we originally split our data set into a train and validation set.\nWith pre-trained weights and a model architecture, you can load a test data set and make predictions. The performance of the model on the test set will guide our decision on whether or not we can use this on out-of-sample data. In our case, we want to know if can we use this model to predict the P-wave for new seismic traces.\nKeras makes leading in models and making predictions easy to do and I’ll go into detail in a future post of how to do this. Conclusion In this tutorial we covered how to compile, train, and make a surface-level evaluation of a baseline convolutional neural network. In the first part of the tutorial we learned how to frame our problem, download data, process it, and save it into a file format from which we can use later for training our convolutional neural network.\nIdentifying the first arriving seismic wave generated by an earthquake is a critical component of earthquake early warning and it is a topic of continued interest from both a scientific and public safety point of view. As of writing this post, earthquake early warning recently received funding to support operations, improve existing seismic stations, and expand the current earthquake early warning system across the West Coast.\nWhile there exist non-machine learning earthquake detection systems, it remains to be seen if deep neural networks will play a role in EEW. Deep learning algorithms not only need to classify and detect earthquakes, but they need to do so accurately and faster than current non-Deep Learning algorithms.\nYou can find the source code to this tutorial on my GitHub.\nSuggested Reading Material  Géron, A. (2017). Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques for Building Intelligent Systems. O\u0026rsquo;Reilly UK Ltd.\n Stanford CS class CS231n: Convolutional Neural Networks for Visual Recognition.\n Ackermann, Nils. \u0026ldquo;Introduction to 1D Convolutional Neural Networks in Keras for Time Sequences\u0026rdquo; Medium, 04 September 2018, https://blog.goodaudience.com/introduction-to-1d-convolutional-neural-networks-in-keras-for-time-sequences-3a7ff801a2cf\n  ","date":1569891600,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1569955397,"objectID":"0f00ba8484db6c7d9d16b92148686461","permalink":"https://ngrayluna.github.io/post/p-phase-picker-tutorial/","publishdate":"2019-10-01T01:00:00Z","relpermalink":"/post/p-phase-picker-tutorial/","section":"post","summary":"In Part I we covered the first steps of our machine learning pipeline: Framing the Problem, Retrieving the Data , Exploring and Understanding your Data, and Processing the Data for training .\nIn this tutorial we will go sraight into compiling, training, and evaluating a baseline convolutional neural network. We\u0026rsquo;ll end by going over what still needs to be done before we can consider this model ready for deployment.","tags":[],"title":"Identifying Seismic Waves with Convolutional Neural Networks [Part II]","type":"post"},{"authors":[],"categories":[],"content":"Table of Contents  Frame the Problem Get the Data Explore the Data Prepare the Data for Training A Non Machine Learning Baseline Machine Learning Baseline Building a RNN with Keras A RNN Baseline Extra  The attractive nature of RNNs comes froms our desire to work with data that has some form of statistical dependency on previous and future outputs. This can take the form of text, such as learning how words in a sentence should flow. It can take the form of timeseries data, such as seismograms, just to name a few. Recurrent neural networks processes sequences by iterating through the sequence of elements and maintaining a state containing information relative to what it has seen so far. [Chollet, F. (2017)]\nWith our basic understanding of RNNs, it\u0026rsquo;s time to dive into a small examples using real timeseries data.\nLet\u0026rsquo;s remind ourselves what the general machine learning workflow is so that we don\u0026rsquo;t get lost:\nFrame the Problem \u0026ndash;\u0026gt; Get the Data \u0026ndash;\u0026gt; Explore the Data \u0026ndash;\u0026gt; Prepare the Data \u0026ndash;\u0026gt; Short-List Promising Models \u0026ndash;\u0026gt; Fine-Tune the System\nWe won\u0026rsquo;t have time to go through each of these steps in detail, but I encourage you to read Chapter 2 from Géron\u0026rsquo;s book [see Suggested Reading below].\n# Run this first import os import numpy as np import matplotlib.pyplot as plt %matplotlib inline from keras.models import Sequential from keras import layers from keras.optimizers import RMSprop  Frame the Problem We want to know if it is possible to predict the temperature in the future? For a given day, we want to look back some days (i.e. time steps) and use said information to predict the weather in the future. We\u0026rsquo;ll use the following variables to contrain what we mean by \u0026lsquo;past\u0026rsquo; and \u0026lsquo;future\u0026rsquo;:\nlook back - number of timesteps to look back from\ndelay - number of timesteps in the future\nsteps - our sample rate\nIn our case that we will set look back = 1440 (1 day consists of 1440 minutes), step = 6 (one data point per hour), and delay = 1440 (one day in the future).\n# How many timesteps back the input data should go. # Note: 1 day is 1440 minutes lookback = 1440 # The period, in timesteps, at which you sample data. Set to 6 in # order to draw one data point every hour. step = 6 # How many timesteps in the future the target should be. delay = 1440  Get the Data Our data set is a timeseries from the Weather Station at the Max Planck Institute for Biogeochemistry in Jena, Germany. Lucky for us, this is already in a format we can quickly and easily work with. We just need to read it in, store it in a workable format, and do some very basic data processing.\nDownload the data and store it in your working directory. Once you have done that, execute this block:\ndata_dir = './' fname = os.path.join(data_dir, 'jena_climate_2009_2016.csv') f = open(fname) data = f.read() f.close()  Since we are working with a .csv file we first need to read it in, and then beat it into something we can work with. Depending on how large your data set is (both row and column-wise) you might want to invest learning using Pandas. It is a library that allows for easy data manipulation and works great for .csv, .xlsx, and .txt sort of data file types.\nWe only have a few thousand samples (420551, to be exact), so we will just use some build-in Python functions:\nlines = data.split('\\n') header = lines[0].split(',') lines = lines[1:] print(\u0026quot;Our header information: \u0026quot;,'\\n') print(header)  Our header information: ['\u0026quot;Date Time\u0026quot;', '\u0026quot;p (mbar)\u0026quot;', '\u0026quot;T (degC)\u0026quot;', '\u0026quot;Tpot (K)\u0026quot;', '\u0026quot;Tdew (degC)\u0026quot;', '\u0026quot;rh (%)\u0026quot;', '\u0026quot;VPmax (mbar)\u0026quot;', '\u0026quot;VPact (mbar)\u0026quot;', '\u0026quot;VPdef (mbar)\u0026quot;', '\u0026quot;sh (g/kg)\u0026quot;', '\u0026quot;H2OC (mmol/mol)\u0026quot;', '\u0026quot;rho (g/m**3)\u0026quot;', '\u0026quot;wv (m/s)\u0026quot;', '\u0026quot;max. wv (m/s)\u0026quot;', '\u0026quot;wd (deg)\u0026quot;']  Note that we seperated the header titles and stored them in the \u0026ldquo;header\u0026rdquo; variable. We also made sure to remove the first column in our floats which contained the \u0026ldquo;Date Time\u0026rdquo; values. [Line #3]\nNow that we have read our data in, let\u0026rsquo;s parse it and store it into a Numpy array so we can easily access the float values later:\nParse the Data into a Numpy Array float_data = np.zeros((len(lines), len(header) - 1)) for i, line in enumerate(lines): values = [float(x) for x in line.split(',')[1:]] float_data[i, :] = values  I like to do sanity checks along the way, so let\u0026rsquo;s make sure our dimensions make sense:\nprint(float_data.shape)  (420551, 14)  We have 420551 time steps (done every 10 minutes), with 14 columns/features. I.e. we created a 2D tensor with dimensions: (# of samples, features).\nGreat. So far so good.\nExplore the Data It\u0026rsquo;s always a good idea to get a sense of what our data looks like. Data exploration is a necessary step to get a broad sense of what kind of data you are working with and to get an overall \u0026lsquo;feel\u0026rsquo; for the data. Depending on the data type, you might want to make histograms or scatter plots for each numerical attribute. You could also compute some basic statistical attributes of your data set such as the mean, standard deviations, minimum and maximum values, and percentiles. (You can take advantage of Pandas\u0026rsquo; built-in function for quick results).\nLet\u0026rsquo;s do a VERY mild peek at our data. Specifically, let\u0026rsquo;s plot the temperature and see how it behaves over time:\ntemp = float_data[:, 1] # temperature (in degrees Celsius) fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(14,5)) ax.plot(range(len(temp)), temp) ax.set_ylabel('Degrees [C]') ax.set_xlabel('Time [min]') plt.show()  As one might expect (or hope to expect), we see a periodic temperature pattern over the years.\nPrepare the Data for Training Depending on what your data set is, you might consider using transformation Pipelines for preprocessing your data. In our case, we want to take care of the fact our data has different scales. Neural networks don\u0026rsquo;t perform well if their scales are dramatically different. No problem, we\u0026rsquo;ll just normalize our timeseries.\nNote: We only do this to our training data (our first 200000 time steps). So only compute and modify these sampels:\nmean = float_data[:200000].mean(axis=0) float_data -= mean std = float_data[:200000].std(axis=0) float_data /= std  \nSplit Data into Training, Validation, and Testing Always split your data set into a trainig, validation, and a test set. As a rule of thumb: you\u0026rsquo;ll want to split it such that you leave 20% of your data for testing and keep the rest as your training and validation set. The bottom snippet of code might look confusing at first, but all it is is a Python Generator for creating the above mentioned data sets.\n# Generator yields a tuple (samples, targets), where samples is one batch # of input data and targets is the corresponding array of # target temperatures. def generator(data, lookback, delay, min_index, max_index,\\ shuffle=False, batch_size=128, step=6): \u0026quot;\u0026quot;\u0026quot; Parameters ---------- data: array, dtype = floats or integers array containing our data lookback: integer number of timesteps to take backward from current index delay: integer number of timesteps to take forward from current index min_index: integer index value used to set lower bound in data array e.g. array_test[min_index:] max_index: integer index value used to cap data array e.g. array_test[:max_index] shuffle: boolean used to determine whether or not to shuffle data or keep in given order batch_size: integer how many sequences to give per epoch step: integer The period, in timesteps, we sample data with Returns: ------- samples: array, dtype = float or int a single batch of input data targets: array, dtype = float, int, or str a single array of target values (in this case temperature) \u0026quot;\u0026quot;\u0026quot; if max_index is None: max_index = len(data) - delay - 1 i = min_index + lookback while 1: if shuffle: rows = np.random.randint(min_index + lookback, max_index, size=batch_size) else: if i + batch_size \u0026gt;= max_index: i = min_index + lookback rows = np.arange(i, min(i + batch_size, max_index)) i += len(rows) samples = np.zeros((len(rows),lookback // step,data.shape[-1])) targets = np.zeros((len(rows),)) for j, row in enumerate(rows): # range(start, end, steps) # First value you start from a random time spot, then go back one day. # i.e. start: random_time - day, end on random_time, take steps of one hour # Want one sample every hour) --\u0026gt; 1440 minutes / 60 minutes = 240 timesteps indices = range(rows[j] - lookback, rows[j], step) samples[j] = data[indices] samples[j] = data[indices] yield samples, targets  Going back to our problem, we said we wanted to use data going back a certain amount of time (data points in our timeseries) to predict the weather in the future (defined by the variable delay). Let\u0026rsquo;s create the data sets we need to feed our recurrent neural network:\n# How many timesteps back the input data should go. # Note: 1 day is 1440 minutes lookback = 1440 # The period, in timesteps, at which you sample data. Set to 6 in # order to draw one data point every hour. step = 6 # How many timesteps in the future the target should be. delay = 144 # The number of samples per batch. batch_size = 128 # Generate data train_gen = generator(float_data, lookback=lookback, delay=delay,\\ min_index=0, max_index=200000, shuffle=True,\\ step=step, batch_size=batch_size) val_gen = generator(float_data, lookback=lookback, delay=delay,\\ min_index=200001, max_index=300000, step=step,\\ batch_size=batch_size) test_gen = generator(float_data, lookback=lookback, delay=delay,\\ min_index=300001, max_index=None, step=step,\\ batch_size=batch_size) # How many steps to draw from val_gen in order to see the entire validation set. val_steps = (300000 - 200001 - lookback) # How many steps to draw from test_gen in order to see the entire test set. test_steps = (len(float_data) - 300001 - lookback)  Establishing a Common Sense Non Machine Learning Baseline Before running this model you want to establish a baseline. Baslines are useful for a couple of reasons: first, it gives you something to compare the performance of your machine learning algorithm with. If your computationally heavy, machine learning algorithm does not perform better than a simple MAE, well\u0026hellip;then there\u0026rsquo;s not really a point to use a machine learning algorithm; The second reason for creating a baseline: they serve as a sanity check.\nSince we are curious about predicting the temperature in the future, a common sense non machine learning baseline could involve using temperature from the last 24 hours to say something about right now.\nOne of doing this could be using an MAE. Here is a code snippet example:\nnp.mean(np.abs(preds - targets)) And the associated code snippet for the entire loop: batch_maes = [] for step in range(val_steps): samples, targets = next(val_gen) preds = samples[:, -1, 1] mae = np.mean(np.abs(preds - targets)) batch_maes.append(mae) print(np.mean(batch_maes))  If you compute the standard deviation of the temperature and run the code snippet above, you\u0026rsquo;ll be get an MAE = 0.29 . This number obviously doesn\u0026rsquo;t make any intuitive sense because our data was normalized to be centered at 0 and have a standard deviation of 1. Translating this into something more understandable, we have an absolute error of:\n0.29 * temperature_std = 2.57°C\nWhich is not terrible, but it\u0026rsquo;s not great. We hope that our machine learning approach will better. Let\u0026rsquo;s find out.\nA Basic Machine Learning Approach I would suggest trying to execute the next block but then interrupting the kernal once you get a flavor of how long it\u0026rsquo;ll take to run.\nBecause of this shortcoming, I\u0026rsquo;ve provided screen shots for the remaining part of the notebook of what will show up if you were to run this:\nmodel = Sequential() # input shape: (240, 14) model.add(layers.Flatten(input_shape=(lookback // step, float_data.shape[-1]))) model.add(layers.Dense(32, activation='relu')) model.add(layers.Dense(1)) model.compile(optimizer=RMSprop(), loss='mae') history = model.fit_generator(train_gen, steps_per_epoch=100, epochs=20, validation_data=val_gen, validation_steps=val_steps)  \nInterpreting the Results of a Basic Machine Learning Algorithm Let\u0026rsquo;s plot the loss curves for both the training and validation data sets:\nloss = history.history['loss'] val_loss = history.history['val_loss'] epochs = range(len(loss)) plt.figure() plt.plot(epochs, loss, 'bo', label='Training loss') plt.plot(epochs, val_loss, 'b', label='Validation loss') plt.title('Training and validation loss') plt.legend() plt.show()  What do we see? On the x-axis we one again have epochs and the y-axis we see the output of our loss function, the mean absolute error. Looking at the training loss we note that with increasing epoch our loss is going down. i.e. the function we are trying to minimize is actually being minimized.\nBut, we can\u0026rsquo;t celebrate just yet. Arguably more important is understanding our validation loss. We would hope that it too would have smaller loss values with each iteration. Instead, we see that our validation loss is increasing after iteration 5.\nA Blurb on the Hypothesis Space Interesing enough, our \u0026lsquo;common sense\u0026rsquo; approach earlier gave us a MAE of 0.29. Which, comparing to our results, is actually better. Why is this? More specifically, why didn\u0026rsquo;t we find the same hypothesis used for the common sense approach?\nRemember that the overall objective of a machine learning algorithm is to find the mapping function/hypothesis between an independent variable X and a dependent variable y such that it best minimizes the cost function. The hypothesis space, where our mapping function lives, of our machine learning network is the space of all possible 2-layer networks with the configuration that we defined. So, it may be that the space we just defined with our machine learning algorithm doesn\u0026rsquo;t actually have the hypothesis we originally found.\nLet\u0026rsquo;s now move on to the main purpose of this notebook, using a recurrent neural netork.\nBuilding a RNN with Keras The simplest RNN we can use with Keras is literally called\nmodel.add(SimpleRNN( )) \nFor example:\nfrom keras.models import Sequential from keras import layers from keras.layers import Embedding, Dense, SimpleRNN model = Sequential() model.add(SimpleRNN(10, input_shape=(3, 1))) model.add(Dense(1))  Let\u0026rsquo;s look at this line by line:\nLine 5: Defined our model architect using a Sequential class.\nLine 6: Added our RNN layer (which also serves as our input layer).\nLine 7: Added a fully connected (i.e. Dense) layer as our output layer.\nThe model.summary() function is a convenient way of checking how our deep neural network textually looks like. It provides key information of our architecture such as:\nthe layer type and the order of the layers from input (first row) to output (bottom row before the \u0026lsquo;=\u0026rsquo;);\nthe shape of the tensor for each output (and thus, what is going into the next layer);\nand the number of weights (here labeled \u0026lsquo;parameters\u0026rsquo;) per layer along with a summary of the total number of weights.\nFor example:\nmodel.summary()  _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= simple_rnn_1 (SimpleRNN) (None, 10) 120 _________________________________________________________________ dense_1 (Dense) (None, 1) 11 ================================================================= Total params: 131 Trainable params: 131 Non-trainable params: 0 _________________________________________________________________  What do we see? The first line is our header\n[ Layer(type), Output Shape,, and Param # ]\nWhere Output Shape is the shape of the tensor that is leaving our first layer (SimpleRNN) and going into the next layer Dense (i.e. a fully connected layer).\nIn the next line We see that we have an output shape of (None, 10) and 120 Parameters:\nsimple_rnn_1 (SimpleRNN) (None, 10) 120  \nWhat does this mean? When we wrote line 6:\nSimpleRNN(10, input_shape=(3, 1))\nWe specified that we had 10 weights (parameters) and input shape of (3,1). The 3 here means we have 3 sequences(e.g. three timeseries points) we want to input and 1 featuere (e.g. Temperature).\nThe formula for the number of parameters/weights:\nParameters = num_weights x num_weights + num_features x num_weights + biases\n = $10 * 10 + 1 * 10 + 10 = 120$\nNote: Full explaination of the parameters below\nFinally, we have our output layer. In this example we defined it as a Dense layer:\nDense(1) \nSo this last Dense layer takes its input (10 (the output of the previous layer) and adds the bias to give us 11 parameters/weights. Since we defined the dense layer as: Dense(1) we are telling our neural network that we want a single output.\nA Recurrent Neural Network Baseline Just like in our previous notebook we\u0026rsquo;ll create our deep neural network by first defining our model architecture with Keras\u0026rsquo; Sequential class. We\u0026rsquo;ll make a key change in that instead of using a SimpleRNN we\u0026rsquo;ll use a GRU layer. I won\u0026rsquo;t go into the details here (if you are curious to learn more, I would suggest reading this blog post) of what GRUs are for brevity. But basically, GRUs are a variation of our Recurrent Neural Network except that it handles the vanishing gradient problem. The link to the paper describing this by Cho, et al. can be found below in the Suggested Reading section.\nmodel = Sequential() model.add(layers.GRU(32, input_shape=(None, float_data.shape[-1]))) # Note that our output layer is defined to have a single value(i.e. a temperature) model.add(layers.Dense(1)) model.compile(optimizer=RMSprop(), loss='mae') history = model.fit_generator(train_gen, steps_per_epoch=500, epochs=20, validation_data=val_gen, validation_steps=val_steps)  What does the above show us? For each epoch Keras is printing out:\n1) How long it took (e.g. 10 seconds in this case)\n2) The loss from the training and validation data sets.\nThe second point here is worth spending some time thinking about. Remember that the overall objective is for us to create an algorithm which learns from the data we give it. i.e. we want our algorithm to generalize to data sets it has never seen before. We should expect, therefore, that the training loss decreases for every epoch. Does this happen in our case? Plotting the training and validation loss will help us understand a litte more of how well our deep neural network did:\n\nInterpreting Results loss = history.history['loss'] val_loss = history.history['val_loss'] epochs = range(len(loss)) plt.figure() plt.plot(epochs, loss, 'bo', label='Training loss') plt.plot(epochs, val_loss, 'b', label='Validation loss') plt.title('Training and validation loss') plt.legend() plt.show()  What do we see? On the x-axis we once again have epochs and the y-axis we see the output of our loss function, the mean absolute error (which we defined when we compiled our RNN in code line 6) . Looking at the training loss we note that with increasing epoch our loss is going down. i.e. the function we are trying to minimize is actually being minimized just like in our basic machine learning approach.\nOnce again, however, our validation loss is increasing after a certain number of epochs.\nThe fact our validation loss is increasing tells us two important aspects of our model: first, our algorithm did not generalize well (i.e. it did not learn) and second, our algorithm was starting to overfit on the training set.\nThat\u0026rsquo;s a bit disappointing\u0026hellip;but don\u0026rsquo;t fret! As it explained in Chollet\u0026rsquo;s Deep Learning with Python[1], there are some tricks we can use to help stop and/or prevent overfitting; nameley, we can take advantage of dropping out, using a regularization, and early stopping. I won\u0026rsquo;t go into the details here, but I would suggest reading Skalski\u0026rsquo;s blog post for more information.\nMore on How the Weights Are Computed for a RNN: From the output above we have 120 parameters. Why do we have 120 parameters?\nRemember, there are two things going on with our simple RNN: First you have the recurrent loop, where the state is fed recurrently into the model to generate the next step. Weights for the recurrent step are:\nrecurrent_weights = num_units * num_units\nSecond, there is a new input of your sequence at each step: input_weights = num_features * num_units\nSo now we have the weights, whats missing are the biases - for every unit one bias: biases = num_units * 1\nIn our case we have that num_units = $10$ and num_features = $1$.\nPutting this altogether we have the following formula for the number of parameters/weights:\nParameters = num_units x num_units + num_features x num_units + biases\nWhere num_units is the number of weights in the RNN (10) and num_features is the number features of our input. (In thie case 1).\nParameters = $10 * 10 + 1 * 10 + 10 = 120$\nAbout this Notebook The above example and code is from Ch.6 of Chollet\u0026rsquo;s Deep Learning with Python[1]. Content was added for further clarification and readability.\nSuggested Reading Material  Cho, et al. (2014), \u0026ldquo;Learning Phrase Representations using RNN Encoder–Decoder for Statistical Machine Translation\u0026rdquo; Association for Computational Linguistics https://arxiv.org/pdf/1406.1078v3.pdf\n Géron, A. (2017). Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques for Building Intelligent Systems. O\u0026rsquo;Reilly UK Ltd.\n Karpath, Andrej. \u0026ldquo;The Unreasonable Effectiveness of Recurrent Neural Networks\u0026rdquo; Andrej Karpathy blog, 24 March 2017, http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n Kostadinov, Simeon. \u0026ldquo;Understanding GRU Networks\u0026rdquo; Towards Data Science, 16 Dec. 2017, https://towardsdatascience.com/understanding-gru-networks-2ef37df6c9be\n Skalski, P. \u0026ldquo;Preventing Deep Neural Network from Overfitting\u0026rdquo; Towards Data Science, 7 Sept. 2018 https://towardsdatascience.com/preventing-deep-neural-network-from-overfitting-953458db800a\n  Sources [1]Chollet, F. (2017). Deep Learning with Python. Manning Publications.\n","date":1567717231,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1567717231,"objectID":"97d5fa05b9fbcd0a4ce19e2c446aa2f0","permalink":"https://ngrayluna.github.io/post/rnn_wkeras/","publishdate":"2019-09-05T14:00:31-07:00","relpermalink":"/post/rnn_wkeras/","section":"post","summary":"Table of Contents  Frame the Problem Get the Data Explore the Data Prepare the Data for Training A Non Machine Learning Baseline Machine Learning Baseline Building a RNN with Keras A RNN Baseline Extra  The attractive nature of RNNs comes froms our desire to work with data that has some form of statistical dependency on previous and future outputs. This can take the form of text, such as learning how words in a sentence should flow.","tags":[],"title":"Training a Recurrent Neural Network Using Keras","type":"post"},{"authors":[],"categories":[],"content":"Table of Contents  What is a RNN \u0026amp; How Do They Work? Writting a RNN with NumPy Building a DNN with Keras  \nWhat is a Recurrent Neural Network and How Do They Work?\nNeural networks data as independent, isolated events. In other words, we don’t treat and/or make use of sequential data. Therefore, in order to process a time-series data (e.g. accelerometer data from a seismometer) or a sequence of events (e.g. text) you would have to feed the entire sequence to the neural network at once!\nThis doesn’t make sense both on a computation-level and a human-level. Think about it, as you read text you are storing a subset of this text in your short-term memory; you are keeping the sequence of words that are relevant for your understanding of the sentence.\nThis is the idea behind Recurrent Neural Networks. A recurrent neural network (RNN) processes sequences by iterating through the sequence of elements and maintaining a state containing information relative to what it has seen so far. RNNs are called recurrent because they perform the same task for every element of a sequence, with the output being dependent on the previous computations.\nIn other words, data points are no longer processed in a single step. The network will loop over itself until it performs the same task on each element on the sequence. The RNN will reset itself only when it reaches the final element.\nLet\u0026rsquo;s visualize this before going through an example. Below we see a typical RNN:\nLeft A single recurrent network, which is nothing more than a network with a loop. Right  The same RNN but unrolled for visualization purposes.\n$x_t$ and $s_t$ are the input and hidden state (both vectors), respectively, at time $t$. Matrices $U$, $V$, and $W$ are the parameters we want to learn from our data. And $o_t$ is the output vector computed using only the hidden state at time $t$. The hidden state, $s_t$, is calculated based on the previous hidden state ($s_t-1$) and the input at the current step, $x_t$:\n$$s_t = f(U * x_t + W * s_t-1)$$\ni.e. $s_t$ kept information on what happened from all of the previous steps. It can be thought as a memory object. Note that, unlike other typical neural networks, recurrent neural networks reuse the same parameters (weights) $U$, $V$, and $W$ during the training process. This makes sense since we are performing the same task on each element of the time sequence, $x_t$.\nOur activation function, $f$, is defined either as an $tanh$ or $ReLU$. For example, when $f$ is defined as $tanh$, our hidden state becomes:\n$$s_t = tanh(U * x_t + W * s_t-1)$$\nWritting your own RNN using Numpy Let\u0026rsquo;s do some programming to get a deep (pun intended) understanding of recurrent neural networks. To do this we will create a generic and simple RNN using Numpy. The objective of this exercise it to understand on a basic level how an RNN operates. We will not worry about using real data for now. Instead, let\u0026rsquo;s create random data and feed this to an RNN. We will say that our input vector has $32$ input features (this is what goes into our input layer) and we will have an output vector with $64$ features.\nBelow are the ingredients you\u0026rsquo;ll need and some psuedo code to get you started.\nRNN ingredients:\n1] Define the dimension of the input space. [This it the input layer of our neural network].\n2] Define the dimension of the output feature space [Output layer of our neural networt].\n3] Generate random noise as our input ‘data’ [We just want to get an idea of HOW this works].\nHint: Input vector should have dimensions (timesteps X input_features)\n4] Define an initial state, $s_{t}$, of the RNN.\n5] Create (random) weight matrices, $W$ and $U$.\n6] Create a for loop. that takes in the input with the current state (the previous output) to obtain the current output.\n Hint: Don\u0026rsquo;t forget about our activation function, $f$.\n7] Update the state for the next step.\n8] The final output should be a 2D tensor with dimensions (timesteps, output_features).\n\nPseudocode for RNN\n# For the first timestep, the previous output isn’t defined; # Thus, our initial state is set to zero. state_t = 0 #Iterates over sequnce elements for input_t in input_sequence: output_t = f(input_t, state_t) # The previous output becomes the state for the next iteration. state_t = output_t  \nHow this would look like in Python:\n### Create fake data and store them as a tensor: # Number of timesteps in the input sequence. timesteps = 100 # Dimensionality of the input feature space input_features = 32 # Dimensionality of the output feature space. output_features = 64 # Input data is random for the sake of it inputs = np.random.random((timesteps, input_features)) ### RNN ### # Initialize state: an all-zero vector state_t = np.zeros((output_features)) # The RNN's parameters are two matrices W and U and a bias vector. # Initialize random weight matrices W = np.random.random((output_features, input_features)) U = np.random.random((output_features, output_features)) b = np.random.random((output_features,)) successive_outputs = [] for input_t in inputs: # Combines the input with the current state (the previous output) # to obtain the current output output_t = np.tanh( np.dot(W, input_t) + np.dot(U, state_t) + b ) # Stores this output in a list successive_outputs.append(output_t) # Use for the next round: state_t = output_t # The final output is a 2D tensor of shape (timesteps, output features) final_output_sequence = np.concatenate(successive_outputs, axis = 0)  Building a DNN in Keras Before jumping in to defining an RNN using Keras, let\u0026rsquo;s remind ourselves what pieces we need to compile DNN using Keras\u0026rsquo; high-level neural network API. The ingredients are:\n Define a model. In TensorFlow there are two ways to do this: first, by using a Sequential class or second, with a functional API (which allows you to build arbritrary model structures). Given that the most common neural network configuration is made up of linear stacks, you\u0026rsquo;ll most likely use the first method more often.\nDefine your DNN by stacking layers We start from the input layer and sequentially add more layers: the hidden layers, and the output layer.(Hence, where the Class name came from). Basically, the more layers you stack together, the more complex model you define (but at the potential expense of overfitting and/or long computation times!).\nOne thing to keep in mind: each layer you define needs to be compatible with the next. In other words, each layer will only accept and return tensors of a certain shape.\nCompile Before the training occurs, you need to define compile the model. To do this, use Keras\u0026rsquo; compile method. This method takes in three arguments:\n1) the optimizer - optimization algorithm (e.g. SGD, Adam, etc.); 2) the loss function the optimizer will try to minimize; and 3) a list of metrics (e.g. accuracy).\nAnd that\u0026rsquo;s it! The last step is to train the model using Keras\u0026rsquo; fit function. More on this when we run our own RNN in the next notebook.\nWith the above knowledge fresh in our memory we could replace our Numpy RNN (lines 17 - 39) with this ONE line:\nmodel.add(SimpleRNN( )) \nFor example:\nfrom keras.models import Sequential from keras import layers from keras.layers import Embedding, Dense, SimpleRNN model = Sequential() model.add(SimpleRNN(10, input_shape=(3, 1))) model.add(Dense(1))  Using TensorFlow backend.  Let\u0026rsquo;s look at this line by line:\nLine 5: Defined our model architect using a Sequential class.\nLine 6: Added our RNN layer (which also serves as our input layer).\nLine 7: Added a fully connected (i.e. Dense) layer as our output layer.\nThe model.summary() function is a convenient way of checking how our deep neural network textually looks like. It provides key information of our architecture such as:\nthe layer type and the order of the layers from input (first row) to output (bottom row before the \u0026lsquo;=\u0026rsquo;);\nthe shape of the tensor for each output (and thus, what is going into the next layer);\nand the number of weights (here labeled \u0026lsquo;parameters\u0026rsquo;) per layer along with a summary of the total number of weights.\nFor example:\nmodel.summary()  _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= simple_rnn_1 (SimpleRNN) (None, 10) 120 _________________________________________________________________ dense_1 (Dense) (None, 1) 11 ================================================================= Total params: 131 Trainable params: 131 Non-trainable params: 0 _________________________________________________________________  What do we see? The first line is our header\n[ Layer(type), Output Shape,, and Param # ]\nWhere Output Shape is the shape of the tensor that is leaving our first layer (SimpleRNN) and going into the next layer Dense (i.e. a fully connected layer).\nIn the next line We see that we have an output shape of (None, 10) and 120 Parameters:\nsimple_rnn_1 (SimpleRNN) (None, 10) 120  \nWhat does this mean? When we wrote line 6:\nSimpleRNN(10, input_shape=(3, 1))\nWe specified that we had 10 weights (parameters) and input shape of (3,1). The 3 here means we have 3 sequences(e.g. three timeseries points) we want to input and 1 featuere (e.g. Temperature).\nOK, now to the weights. From the output above we have 120 parameters. Why do we have 120 parameters?\nRemember, there are two things going on with our simple RNN: First you have the recurrent loop, where the state is fed recurrently into the model to generate the next step. Weights for the recurrent step are:\nrecurrent_weights = num_units * num_units\nSecond, there is a new input of your sequence at each step: input_weights = num_features * num_units\nSo now we have the weights, whats missing are the biases - for every unit one bias:\nbiases = num_units * 1\nIn our case we have that num_units = $10$ and num_features = $1$.\nPutting this altogether we have the following formula for the number of parameters/weights:\nParameters = num_units x num_units + num_features x num_units + biases\nWhere num_units is the number of weights in the RNN (10) and num_features is the number features of our input. (In thie case 1).\nParameters = $10 * 10 + 1 * 10 + 10 = 120$\nFinally, we have our output layer. In this example we defined it as a Dense layer:\nDense(1) \nSo this last Dense layer takes its input (10 (the output of the previous layer) and adds the bias to give us 11 parameters/weights. Since we defined the dense layer as: Dense(1) we are telling our neural network that we want a single output.\nGreat! So you now how they work and you just looked through a short example of how you could implement a RNN model using Keras. In the next notebook we will do a full runthrough of creating and running a RNN on real data.\nJust as an aside, while the RNN we defined was cute and simple, in practice these simple RNNs are not used. The reason? Well, we run into the problem of vanishing gradients and exploding gradients. The vanishing gradient problem is why folks use the more exotic recurrent neural network known as long short term memory (LSTM). I won\u0026rsquo;t go over the details here, but you can think of LSTMs as an extended version of recurrent neural networks. LSTMs act like computers in that they can read, delete, and add to their \u0026lsquo;stored\u0026rsquo; memory. This allows them to \u0026lsquo;extend\u0026rsquo; their \u0026lsquo;short term memory\u0026rsquo; to \u0026lsquo;longer short term memory\u0026rsquo;.\nIf you are curious, I suggest checking this blog by Madhu Sanjeevi. It\u0026rsquo;s one of my favorite explainations of LSTMs.\nAbout this Notebook The above example and code is from Ch.6 of Chollet\u0026rsquo;s Deep Learning with Python[1]. Content was added for further clarification and readability.\nJupyter Notebook by: Noah Luna\nSuggested Reading Material  Géron, A. (2017). Hands-On Machine Learning with Scikit-Learn and TensorFlow: Concepts, Tools, and Techniques for Building Intelligent Systems. O\u0026rsquo;Reilly UK Ltd.\n Karpath, Andrej. \u0026ldquo;The Unreasonable Effectiveness of Recurrent Neural Networks\u0026rdquo; Andrej Karpathy blog, 24 March 2017, http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n Sanjeevi, Madhu. \u0026ldquo;DeepNLP — LSTM (Long Short Term Memory) Networks with Math\u0026rdquo; Medium, 21 Jan 2018, https://medium.com/deep-math-machine-learning-ai/chapter-10-1-deepnlp-lstm-long-short-term-memory-networks-with-math-21477f8e4235\n  Sources  [1]Chollet, F. (2017). Deep Learning with Python. Manning Publications.  ","date":1554080400,"expirydate":-62135596800,"kind":"page","lang":"en","lastmod":1565894597,"objectID":"da36a6760edd697343507d6899dc9c96","permalink":"https://ngrayluna.github.io/post/rnn_wnumpy/","publishdate":"2019-04-01T01:00:00Z","relpermalink":"/post/rnn_wnumpy/","section":"post","summary":"Table of Contents  What is a RNN \u0026amp; How Do They Work? Writting a RNN with NumPy Building a DNN with Keras  \nWhat is a Recurrent Neural Network and How Do They Work?\nNeural networks data as independent, isolated events. In other words, we don’t treat and/or make use of sequential data. Therefore, in order to process a time-series data (e.g. accelerometer data from a seismometer) or a sequence of events (e.","tags":[],"title":"Code Your Own RNN with NumPy","type":"post"}]